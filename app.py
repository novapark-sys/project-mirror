import streamlit as st
import pandas as pd
import numpy as np
import networkx as nx
from pyvis.network import Network
import tempfile
import asyncio
from datetime import datetime
import math
import torch
try:
    torch.classes.__path__ = []
except Exception:
    pass

# LangChain & OpenAI imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from typing import List

# --- 1. ì„¤ì • ë° UI ì´ˆê¸°í™” ---
st.set_page_config(page_title="Project Mirror", layout="wide")

st.title("ğŸ’ Project Mirror: ê°œì¸ë‹¨ìœ„ í•©ì„±íŒ¨ë„")
st.markdown("""
    Project MirrorëŠ” ê³¼ê±° ì„¤ë¬¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì¡´ ì¸ë¬¼ì„ ì™„ë²½í•˜ê²Œ ëª¨ì‚¬í•˜ëŠ” 'ì´ˆê°œì¸í™” í•©ì„± íŒ¨ë„(Synthetic Panel)' ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ì…ë‹ˆë‹¤.
    ìŠ¤íƒ í¼ë“œ ëŒ€í•™êµì˜ Generative Agents (2023) ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ê¸°ì–µ ì¸ì¶œ ì•Œê³ ë¦¬ì¦˜(Retrieval Function)ê³¼ ì„±ì°°(Reflection) ë©”ì»¤ë‹ˆì¦˜ì„ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì— ë§ì¶° ìµœì í™”í•œ Hybrid RAG ëª¨ë¸ì…ë‹ˆë‹¤.

""")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    st.divider()
    st.header("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì„¤ë¬¸ ë°ì´í„° (CSV)", type=["csv"])
    
    st.divider()
    st.subheader("ğŸ›ï¸ ì¸ì¶œ ê°€ì¤‘ì¹˜ (Retrieval Weights)")
    alpha = st.slider("Recency (ìµœì‹ ì„±)", 0.0, 2.0, 1.0, help="ìµœê·¼ ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ë³¼ ê²ƒì¸ê°€")
    beta = st.slider("Importance (ì¤‘ìš”ë„)", 0.0, 2.0, 1.0, help="ì¸ìƒì˜ ì¤‘ìš”í•œ ì‚¬ê±´ì„ ì–¼ë§ˆë‚˜ ê°€ì¤‘í•  ê²ƒì¸ê°€")
    gamma = st.slider("Relevance (ê´€ë ¨ì„±)", 0.0, 5.0, 3.0, help="ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ ì°¾ì„ ê²ƒì¸ê°€")

    if not openai_api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

# --- 2. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

@st.cache_resource
def get_models(api_key):
    # mini ëª¨ë¸ ì‚¬ìš©
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=api_key)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    return llm, embeddings

def generate_persona_summary(llm, df):
    """
    [Ideal Implementation] 
    ì „ì²´ ë°ì´í„°ë¥¼ ì²­í¬(Chunk)ë¡œ ë‚˜ëˆ„ì–´ 'ì¤‘ê°„ ì„±ì°°(Insight)'ì„ ì–»ê³ ,
    ì´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ 'í•µì‹¬ í˜ë¥´ì†Œë‚˜'ë¥¼ ì¶”ì¶œí•˜ëŠ” Map-Reduce ë°©ì‹
    """
    
    # 1. ë°ì´í„°ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ê±° -> í˜„ì¬ íë¦„ íŒŒì•… ì¤‘ìš”)
    df_sorted = df.sort_values('created_at', ascending=True)
    
    # 2. [Map ë‹¨ê³„] ë°ì´í„°ë¥¼ 50ê°œì”© ì˜ë¼ì„œ 'ì¤‘ê°„ ìš”ì•½(Mini-Reflection)' ìƒì„±
    # (5000ê°œ ë°ì´í„°ë¼ë©´ ì•½ 100ë²ˆì˜ LLM í˜¸ì¶œì´ ë°œìƒí•˜ì§€ë§Œ, ê°€ì¥ ì •í™•í•¨)
    # *ë¹„ìš©/ì†ë„ ì¡°ì ˆì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” 100ê°œ ë‹¨ìœ„ë¡œ strideí•˜ê±°ë‚˜, ì¤‘ìš” ë°ì´í„°ë§Œ í•„í„°ë§ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    # *PoCìš©ìœ¼ë¡œëŠ” ì „ì²´ë¥¼ ë‹¤ ëŒë¦¬ê¸°ë³´ë‹¤, '1ë…„ ë‹¨ìœ„' ë˜ëŠ” '50ê°œ ë‹¨ìœ„'ë¡œ ëŠëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    
    chunk_size = 50
    chunks = [df_sorted[i:i + chunk_size] for i in range(0, len(df_sorted), chunk_size * 3)]
    
    # ì§„í–‰ ìƒí™© í‘œì‹œìš© (Streamlit ì „ìš©)
    progress_text = "ì „ì²´ ìƒì•  ë°ì´í„°ë¥¼ ì •ë…í•˜ë©° ì„±ì°° ì¤‘ì…ë‹ˆë‹¤... (Map-Reduce)"
    my_bar = st.progress(0, text=progress_text)
    
    intermediate_insights = []
    
    for i, chunk in enumerate(chunks):
        text_blob = ""
        for _, row in chunk.iterrows():
            text_blob += f"- [{row['created_at'].strftime('%Y-%m')}] Q:{row['question']} A:{row['answer']}\n"
            
        # ì¤‘ê°„ ì„±ì°° í”„ë¡¬í”„íŠ¸
        map_prompt = f"""
        Analyze these survey records (a part of a person's life).
        Extract 3-5 key keywords or short sentences regarding their **personality, values, and changes**.
        
        [Records]
        {text_blob}
        """
        # ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë¹„ë™ê¸°ë¡œ ëŒë¦¬ê±°ë‚˜, ì—¬ê¸°ì„œ invoke
        insight = llm.invoke(map_prompt).content
        intermediate_insights.append(insight)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        my_bar.progress((i + 1) / len(chunks), text=f"{i+1}/{len(chunks)} êµ¬ê°„ ë¶„ì„ ì¤‘...")
    
    my_bar.empty() # ë°” ì œê±°

    # 3. [Reduce ë‹¨ê³„] ì¤‘ê°„ ìš”ì•½ë“¤ì„ ëª¨ì•„ì„œ 'ìµœì¢… í˜ë¥´ì†Œë‚˜' ìƒì„±
    # ì¤‘ê°„ ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ì´ê²ƒë„ ë‹¤ì‹œ ìª¼ê°œì•¼ í•˜ì§€ë§Œ, ë³´í†µì€ Context Window ë‚´ì— ë“¤ì–´ì˜µë‹ˆë‹¤.
    all_insights = "\n".join(intermediate_insights)
    
    final_prompt = f"""
    Below are the chronological insights extracted from a person's entire survey history (from past to present).
    Synthesize these into a **comprehensive 'Core Persona'**.
    
    [Chronological Insights]
    {all_insights}
    
    [Instruction]
    1. Identify consistent traits (Immutable Core).
    2. Note any changes in values or lifestyle over time (Evolution).
    3. Summarize into a structured profile in **Korean**:
       - **ê¸°ë³¸ ì„±í–¥:** (Personality & Tone)
       - **í•µì‹¬ ê°€ì¹˜ê´€:** (Values)
       - **ì†Œë¹„/ë¼ì´í”„ìŠ¤íƒ€ì¼:** (Spending & Life)
       - **ì£¼ìš” ë³€í™”:** (Life trajectory)
    """
    
    return llm.invoke(final_prompt).content

def search_candidates_from_full_data(df, query, limit=30):
    """[Broad Search] ì „ì²´ ë°ì´í„° ëŒ€ìƒ í‚¤ì›Œë“œ 1ì°¨ í•„í„°ë§"""
    keywords = query.split()
    keywords = [w for w in keywords if len(w) >= 2] # 2ê¸€ì ì´ìƒë§Œ
    
    candidates = pd.DataFrame()
    if keywords:
        # ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
        mask = df['question'].astype(str).str.contains('|'.join(keywords), case=False) | \
               df['answer'].astype(str).str.contains('|'.join(keywords), case=False)
        candidates = df[mask]
    
    # í›„ë³´ê°€ ë¶€ì¡±í•˜ë©´ ìµœì‹  ë°ì´í„°ë¡œ ì±„ì›€ (Context ìœ ì§€)
    if len(candidates) < limit:
        needed = limit - len(candidates)
        recent_df = df.head(needed)
        candidates = pd.concat([candidates, recent_df]).drop_duplicates()
        
    return candidates.head(limit)

async def evaluate_importance(llm, text):
    """[Importance] ê¸°ì–µì˜ ì¤‘ìš”ë„ í‰ê°€ (1~10)"""
    try:
        res = await llm.ainvoke(f"Rate importance (1-10) of this memory: '{text}'. Return number only.")
        val = int(''.join(filter(str.isdigit, res.content)))
        return min(max(val, 1), 10)
    except: return 5

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

async def calculate_scores_and_rank(candidates_df, query, llm, embeddings_model, weights):
    """[Deep Scoring] ë…¼ë¬¸ ìˆ˜ì‹ ì ìš© ($Score = R + I + R$)"""
    texts = [f"Q: {row['question']} / A: {row['answer']}" for _, row in candidates_df.iterrows()]
    
    # 1. ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬: ì¤‘ìš”ë„ í‰ê°€ & ì„ë² ë”©
    importance_tasks = [evaluate_importance(llm, t) for t in texts]
    
    importances, query_vector, doc_vectors = await asyncio.gather(
        asyncio.gather(*importance_tasks),
        embeddings_model.aembed_query(query),
        embeddings_model.aembed_documents(texts)
    )
    
    scored_memories = []
    current_time = datetime.now()
    alpha, beta, gamma = weights
    
    for i, (_, row) in enumerate(candidates_df.iterrows()):
        # A. Recency (Exponential Decay)
        # ì‹¤ì œ ë°ì´í„°ê°€ ê³¼ê±° ë°ì´í„°ì´ë¯€ë¡œ, í˜„ì¬ ì‹œì ê³¼ì˜ ì°¨ì´(ì¼ ë‹¨ìœ„) ê³„ì‚°
        days_diff = abs((current_time - row['created_at']).days)
        # 1ë…„(365ì¼) ì§€ë‚˜ë©´ ì ìˆ˜ê°€ ì•½ 0.5ë°°ê°€ ë˜ë„ë¡ ê°ì‡  (0.998)
        recency = math.pow(0.998, days_diff)
        
        # B. Importance (Normalized 0~1)
        importance = importances[i] / 10.0
        
        # C. Relevance (Cosine Similarity)
        relevance = cosine_similarity(query_vector, doc_vectors[i])
        
        # D. Final Score
        score = (alpha * recency) + (beta * importance) + (gamma * relevance)
        
        scored_memories.append({
            "text": texts[i], 
            "score": score,
            "details": {
                "Recency(ìµœì‹ )": round(recency, 2), 
                "Importance(ì¤‘ìš”)": round(importance, 2), 
                "Relevance(ê´€ë ¨)": round(relevance, 2)
            },
            "created_at": row['created_at']
        })
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ Top 5 ë°˜í™˜
    scored_memories.sort(key=lambda x: x['score'], reverse=True)
    return scored_memories[:10]

def visualize_brain_map(top_memories, persona_summary):
    """[Visualization] ì‚¬ê³  ê³¼ì • ì‹œê°í™” (Brain Map)"""
    net = Network(height="500px", width="100%", bgcolor="#ffffff", font_color="black", notebook=False)
    
    # ì¤‘ì‹¬ ë…¸ë“œ (User) - í˜ë¥´ì†Œë‚˜ ìš”ì•½ í¬í•¨
    user_label = "User (Persona)"
    net.add_node(user_label, label=user_label, title=persona_summary, color="#1f77b4", size=30)
    
    # ê¸°ì–µ ë…¸ë“œ ì—°ê²°
    for i, mem in enumerate(top_memories):
        mem_id = f"Memory_{i+1}"
        # ì ìˆ˜ì— ë”°ë¼ ë…¸ë“œ í¬ê¸° ì¡°ì ˆ
        size = 15 + (mem['score'] * 5)
        
        # íˆ´íŒì— ìƒì„¸ ì •ë³´ í‘œì‹œ
        tooltip = f"Total Score: {mem['score']:.2f}\n\n[Details]\n{mem['details']}\n\n[Content]\n{mem['text']}"
        
        # ìƒ‰ìƒ: ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì§„í•œ ìƒ‰ (Orange -> Red)
        color = "#ff7f0e" if mem['score'] < 3.0 else "#d62728"
        
        net.add_node(mem_id, label=f"Memory {i+1}", title=tooltip, color=color, size=size)
        
        # ì—£ì§€: ë‘ê»˜ë¥¼ ì ìˆ˜ì— ë¹„ë¡€í•˜ê²Œ
        net.add_edge(user_label, mem_id, width=mem['score'], color="#cccccc")

    # ë¬¼ë¦¬ ì—”ì§„ ì„¤ì •
    net.force_atlas_2based()
    
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".html", mode='w', encoding='utf-8') as f:
            net.save_graph(f.name)
            return f.name
    except: return None

def generate_final_answer(llm, query, top_memories, persona_summary):
    """[Generation] CoT ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    memory_text = "\n".join([f"- {m['text']}" for m in top_memories])
    
    prompt = f"""
    You are a simulation of a real person based on survey data.
    
    [Core Persona]
    {persona_summary}
    
    [Retrieved Memories (Top Scored)]
    {memory_text}
    
    [Question]
    {query}
    
    [Instruction]
    1. **Think Step-by-Step:** Analyze how your 'Core Persona' and specific 'Memories' relate to the question.
    2. **Resolve Conflicts:** If recent memories conflict with old persona, follow the recent behavior (refer to dates in memory).
    3. **Tone & Style:** Mimic the tone found in the memories (short/long, formal/casual).
    4. **Answer:** Respond in the first person. Explain your reasoning clearly.
    """
    return llm.invoke(prompt).content

# --- 3. ë©”ì¸ ì‹¤í–‰ ---

# ì„¸ì…˜ ì´ˆê¸°í™”
if 'persona_summary' not in st.session_state:
    st.session_state['persona_summary'] = None

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file)
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='cp949')
        
    # [í•„ìˆ˜] created_atì„ datetime ê°ì²´ë¡œ ë³€í™˜ (Recency ê³„ì‚°ìš©)
    if 'created_at' in df.columns:
        df['created_at'] = pd.to_datetime(df['created_at'])
        df = df.sort_values('created_at', ascending=False)
    else:
        st.error("ì˜¤ë¥˜: CSV íŒŒì¼ì— 'created_at' ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    # 1. í˜ë¥´ì†Œë‚˜ ìƒì„± (ìµœì´ˆ 1íšŒ)
    if st.session_state['persona_summary'] is None:
        with st.spinner("ğŸ•µï¸ ì „ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 'í•µì‹¬ í˜ë¥´ì†Œë‚˜'ë¥¼ ì¶”ì¶œ ì¤‘..."):
            llm_base, _ = get_models(openai_api_key)
            st.session_state['persona_summary'] = generate_persona_summary(llm_base, df)
            st.success("í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì™„ë£Œ!")
            
    with st.sidebar.expander("ğŸ‘¤ ë¶„ì„ëœ í˜ë¥´ì†Œë‚˜", expanded=True):
        st.info(st.session_state['persona_summary'])

    # 2. ì¸í„°ë·°
    st.subheader("ğŸ’¬ AI íŒ¨ë„ ì‹¬ì¸µ ì¸í„°ë·°")
    
    col_q, col_btn = st.columns([4, 1])
    with col_q:
        query = st.text_input("ì§ˆë¬¸ ì…ë ¥", label_visibility="collapsed", placeholder="ì˜ˆ: ì´ íŒ¨ë„ì€ ìƒˆë¡œìš´ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ì‹ ì²­í• ê¹Œìš”?")
    with col_btn:
        run_btn = st.button("ì˜ˆì¸¡ ì‹¤í–‰", use_container_width=True)
    
    if query and run_btn:
        llm, embed_model = get_models(openai_api_key)
        
        # A. ê²€ìƒ‰ & ì±„ì 
        with st.spinner("1. ê¸°ì–µ ì¸ì¶œ ë° ì •ë°€ ì±„ì  ì¤‘ ($R+I+R$)..."):
            # 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° ëŒ€ìƒ í‚¤ì›Œë“œ ê²€ìƒ‰
            candidates = search_candidates_from_full_data(df, query, limit=30)
            
            # 2ë‹¨ê³„: ë…¼ë¬¸ ê³µì‹ ì ìš© (ë¹„ë™ê¸°)
            top_memories = asyncio.run(
                calculate_scores_and_rank(candidates, query, llm, embed_model, (alpha, beta, gamma))
            )
            
        # B. ë‹µë³€ & ì‹œê°í™”
        with st.spinner("2. ë‹µë³€ ìƒì„± ë° ë‡Œ êµ¬ì¡° ì‹œê°í™” ì¤‘..."):
            # 3ë‹¨ê³„: ë‹µë³€ ìƒì„±
            answer = generate_final_answer(llm, query, top_memories, st.session_state['persona_summary'])
            # 4ë‹¨ê³„: ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            graph_html = visualize_brain_map(top_memories, st.session_state['persona_summary'])
            
        # C. ê²°ê³¼ í™”ë©´
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("### ğŸ¤– AI ë‹µë³€")
            st.write(answer)
            st.markdown("---")
            st.caption("ğŸ’¡ ìš°ì¸¡ ê·¸ë˜í”„ëŠ” AIê°€ ë‹µë³€ì„ ìœ„í•´ í™œì„±í™”í•œ 'ìƒê°ì˜ ì§€ë„'ì…ë‹ˆë‹¤.")

        with col2:
            st.markdown("### ğŸ§  ì‚¬ê³ ì˜ ì§€ë„ (Brain Map)")
            if graph_html:
                with open(graph_html, 'r', encoding='utf-8') as f:
                    st.components.v1.html(f.read(), height=400)
            
            with st.expander("ğŸ” ê¸°ì–µë³„ ìƒì„¸ ì ìˆ˜í‘œ (Evidence)"):
                for m in top_memories:
                    score_info = f"Score: {m['score']:.2f} (R:{m['details']['Recency(ìµœì‹ )']} / I:{m['details']['Importance(ì¤‘ìš”)']} / Rel:{m['details']['Relevance(ê´€ë ¨)']})"
                    st.markdown(f"**{score_info}**")
                    st.caption(f"ğŸ“ {m['text']}")
                    st.markdown("---")

else:
    st.info("ğŸ‘ˆ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")